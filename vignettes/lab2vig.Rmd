---
title: "Estimating Linear Regression Parameters: The Beta's"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Estimating Linear Regression Parameters: The Beta's}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(MATH5773ROSAlab2)
```

# Preliminary Items and Packages

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(ggpmisc)
spruce <- read.csv("SPRUCE.csv")
fire <- read.csv("fire.csv")
```

# Task 1

## 1

Since 

$$Y_i = \beta_0 + \beta_1x_i + \epsilon$$

where $\epsilon \mathop{\sim}\limits^{\mathrm{iid}} N(0,\sigma^2)$, and considering that the linear predictor terms are held constant, it follows that 

$$Y_i \mathop{\sim}\limits^{\mathrm{iid}} N(\beta_0 + \beta_1x_i, \sigma^2)$$

## 2

The expression for the linear predictor is:

$$
  y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k
$$

## 3

The expression for the design matrix *X* is given by $(1 \ x_1 \ x_2 \ \cdots \ x_k)$. Expressed fully as a matrix, that is

$$
  \begin{align}
    \mathbf{X} = \begin{bmatrix} 
                    1 & x_{11} & x_{12} & \cdots & x_{1k} \\
                    1 & x_{21} & x_{22} & \cdots & x_{2k} \\ 
                    \vdots & \vdots & \vdots & \ddots & \vdots \\ 
                    1 & x_{n1} & x_{n2} & \cdots & x_{nk} \\  
                 \end{bmatrix}
  \end{align}
$$

## 4 `mylm()`

```{r}
mylm <- function(x, y, alpha=0.05) { # x and y are vectors
  x0 <- rep(1, times = length(x))
  df <- data.frame(x0, x)
  dat <- data.frame(x0, x, y)
  
  X <- as.matrix(df)
  Y <- matrix(y, nrow = length(y), ncol = 1)
  
  cc <- solve(t(X) %*% X)
  
  betahat <- cc %*% t(X) %*% Y
  row.names(betahat) <- c("beta0", "beta1")
  
  degfree <- length(x) - 2
  RSS <- t(Y) %*% Y - t(betahat) %*% t(X) %*% Y
  
  ssq <- RSS / degfree
  s <- sqrt(ssq)
  tt <- qt(1-alpha/2, df = degfree)
  
  me0 <- tt*s*sqrt(cc[1,1])
  me1 <- tt*s*sqrt(cc[2,2])
  
  l0 <- betahat[1] - me0
  u0 <- betahat[1] + me0
  ci_beta0 <- c(l0, u0)
  
  l1 <- betahat[2] - me1
  u1 <- betahat[2] + me1
  ci_beta1 <- c(l1, u1)
  
  g <- ggplot(data = dat, aes(x = x, y = y)) +
    geom_point(size = 3.5, color = "green") +
    stat_smooth(method = "lm", formula = y ~ x) +
    stat_poly_eq(
      formula = y ~ x,
      aes(label = paste(after_stat(eq.label), sep = "~~~~")),
      parse = TRUE,
      cex = 5,
      color = "blue") +
    labs(title = "Scatterplot with LSRL")
  print(g)
  
  invisible(
    list(betahat=betahat,
         ci_beta0=ci_beta0, 
         ci_beta1=ci_beta1,
         X=X, 
         Y=Y,
         ssq=ssq
    )
  )
}
```

```{r, fig.align="center", fig.width=7, fig.height=5}
l <- mylm(x = spruce$BHDiameter, y = spruce$Height)
l
```

## 5 

```{r}
ylm <- lm(Height ~ BHDiameter, data = spruce)
summary(ylm)
```

Based on the outputs from `mylm()` and the `summary(ylm)`, both methods produce the same model.

---

# Task 2

![](a.jpg)

---

# Task 3

## 1

```{r, fig.align="center", fig.width=7, fig.height=5}
ylm <- lm(DAMAGE ~ DISTANCE, data = fire)

myrss <- function(x, list = TRUE){ # x a vector
  a <- x[1]
  b <- x[2]
  r <- fire$DAMAGE - (a + b * fire$DISTANCE) 
  r <- as.matrix(r)
  if(list == TRUE){
    l <-  list(r = r, RSS = t(r) %*% r)
  }
  else({
    l <- t(r) %*% r # RSS
  })
  l
}

grd <- expand.grid(a = seq(10, 11,length = 100), b = seq(4, 5, length = 100))
zz <- apply(grd, 1, myrss, list = FALSE)
grd2 <- within(grd, z <- zz)
grd22 <- grd2[which.min(zz), ] # minimize cost

scatterplot3d::scatterplot3d(grd2, 
              xlab  = expression(hat(beta)[0]),
              ylab = expression(hat(beta)[1]),
              zlab = expression(SSE),
              main = "Loss function",
              highlight.3d = TRUE
              )

myrss(coef(ylm),list = TRUE)
```

## 2

The $\beta$ estimates which minimize the `RSS` are:

```{r}
names(grd22) <- c("beta0", "beta1", "RSS_min")
grd22[1:2]
```

## 3

```{r}
coef(ylm)
```

When comparing both outputs, the estimates are very close.

## 4

**Prove**: $\mathbf{P} = \mathbf{X(X'X)^{-1}X'}$, where $\mathbf{P}$ is the projection matrix.

  **Proof**:
  
Recall from Linear Algebra that the shortest distance between a vector $\mathbf{Y}$ and $Col(\mathbf{X})$ is the orthogonal projection of $\mathbf{Y}$ on $Col(\mathbf{X})$. Also recall, if $\mathbf{u \cdot v} = 0$, then $\mathbf{u}$ and $\mathbf{v}$ are orthogonal vectors.

Now 
  
$$\mathbf{Y} = \mathbf{X \beta} + \mathbf{\epsilon}$$
  
and 
  
$$\mathbf{\hat{\epsilon}} = \mathbf{Y-X\hat{\beta}}$$

From the definition of the dot product of orthogonal vectors, we have

$$
  \begin{align}
    \mathbf{X'\hat{\epsilon}} &= 0 \\
    \mathbf{X'(Y-X\hat{\beta})} &= 0 \\
    \mathbf{X'Y - X'X\hat{\beta}} &= 0 \\
    \mathbf{(X'X)\hat{\beta}} &= \mathbf{X'Y} \\
    \mathbf{\hat{\beta}} &= \mathbf{(X'X)^{-1}X'Y}
      
      

  \end{align}
$$
  
Now

$$
  \begin{align}
    \mathbf{PY} &= \mathbf{X\hat{\beta}} \\
    \mathbf{PY} &= \mathbf{X(X'X)^{-1}X'Y} \\
    \mathbf{P}  &= \mathbf{X(X'X)^{-1}X'} \\
  \end{align}
$$

Thus the proof is now complete. $\blacksquare$

## 5

**Show**: $\nabla_{\hat{\beta}} = 2(\mathbf{-X'Y + X'X\hat{\beta}})$

First,

$$
  \begin{align}
    RSS &= (\mathbf{Y-X\hat{\beta}})'(\mathbf{Y-X\hat{\beta}}) \\
        &= (\mathbf{Y'-X' \hat{\beta'}})(\mathbf{Y-X\hat{\beta}}) \\
        &= \mathbf{Y'Y - 2X'\hat{\beta'}Y + X'\hat{\beta'}X\hat{\beta}} 
  \end{align}
$$

Then,

$$
  \begin{align}
    \nabla_{\hat{\beta}}RSS &= 0 \mathbf{-2X'Y + 2X'X\hat{\beta}} \\
                            &= 2(\mathbf{-X'Y + X'X\hat{\beta}}) \\
  \end{align}
$$

Now, to find the minimum, we set the derivative equal to $0$ so that

$$
  \begin{align}
    2(\mathbf{-X'Y + X'X\hat{\beta}}) &= 0 \\
    \mathbf{X'X\hat{\beta}} &= \mathbf{X'Y} \\
    \mathbf{\hat{\beta}} &= \mathbf{(X'X)^{-1}X'Y} \\
  \end{align}
$$

Thus, we have shown that

$$
  \nabla_{\hat{\beta}} RSS = 2(\mathbf{-X'Y + X'X\hat{\beta}})
$$

and that

$$
  \mathbf{\hat{\beta}} = \mathbf{(X'X)^{-1}X'Y}
$$

## 6 `graddesc`

```{r, fig.align="center", fig.width=7, fig.height=5}
mygd <- function(binit = c(), s = 0.00001, iter = 500, ylm = ylmf, x, y){
  
  # Matrices
  x = model.matrix(ylm)
  y = as.matrix(y)

  # BetaHat
  betahat <- solve(t(x) %*% x) %*% t(x) %*% y 

  # ylm
  sm <- summary(lm(y ~ x[, 2]))
  
  # estimated Beta's as matrix
  binit <- as.matrix(binit)
  
  # build stuff
  beta <- matrix(NA, nrow = 2, ncol = iter + 1, dimnames = list(c("B0", "B1")))
  beta[,1] <- binit
  sslope <- vector(mode = "numeric", length = iter + 1)
  
  for(i in 1:iter){
    slope = -2 * t(x) %*% (y - x %*% beta[,i])
    sslope[i] <- sqrt(t(slope) %*% slope)
    beta[,i + 1] <- beta[,i] - s * slope 
  }
  
  beta <- t(beta)

  t0 <- as.vector(tail(beta)[,1])
  t1 <- as.vector(tail(beta)[,2])
  
  Index = c(1:(iter + 1))

  beta.df <- data.frame(Index, beta[,1], beta[,2])
  names(beta.df) <- c("Index", "Intercept", "Slope")
  
  par(mfrow = c(1, 3))

  p1 <- plot(x = beta.df$Index, 
       y = beta.df$Intercept,
       pch = 21,
       col = 'hotpink',
       bg = 'white',
       ylab = "Intercept",
       xlab = "Index",
       type = "b",
       cex = 1.2
       )
  
 abline(h = ylm$coefficients[1], col = 'blue')
 
 p2 <- plot(x = beta.df$Index,
            y = beta.df$Slope,
            pch = 21,
            col = "hotpink",
            bg = "white",
            ylab = "Slope",
            xlab = "Index",
            type = "b",
            cex = 1.2
            )
 
 abline(h = ylm$coefficients[2], col = "blue")
 
 p3 <- plot(x = beta.df$Index,
            y = sslope,
            type = "l",
            col = "hotpink",
            lwd = 1.5,
            ylab = "Gradient of RSS",
            xlab = "Index"
            )
 
 invisible(list(B0.hat_tail = tail(beta.df$Intercept), B1.hat_tail = tail(beta.df$Slope), compare_to_ylm=coef(ylm)))
}

l <- mygd(binit = c(10,4), s = 0.001, iter = 500, ylm = ylm, x = fire$DISTANCE, y = fire$DAMAGE )
l
```

---

# Task 4

```{r}
ylm <- lm(DAMAGE ~ DISTANCE, data = fire)
```

## `lmblast()`

```{r}
lmblast <- function(obj, alpha = 0.05) { # obj is an lm()
  
  # Summary of lm()
  sm <- summary(obj)
  
  # R-Squared
  rsq <- sm$r.squared
  
  # Residuals
  resid <- as.matrix(sm$residuals)
  colnames(resid) <- "Residuals"
  
  # Design Mat (X)
  X <- cbind(1, as.matrix(obj$model[,1]))
  
  # Response Mat (Y)
  Y <- as.matrix(obj$model[,2])
  
  # AIC (Akaike Info Criterion)
  aic <- extractAIC(obj)
  
  # S-Squared (point est for pop var)
  s <- sm$sigma
  ssq <- s^2 
  
  # 100(1 - alpha)% ci for Beta Vector
  ci <- confint(obj, level = 1-alpha)
  ci
  
  # List
  list(R.Squared=rsq, Residuals=round(resid,4), Design.Mat.X=X, Response.Mat.Y=Y, AIC=aic, S.Squared=ssq, Conf.Int=ci, RAR = "What a blast!")
}
lmblast(ylm, alpha = 0.1)
```


---

# Task 5

![](b.jpg)










































